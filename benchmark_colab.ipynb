{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Llama-3.2-1B æ•ˆèƒ½åŸºæº–æ¸¬è©¦ (vLLM for Portfolio)\n",
                "\n",
                "æœ¬ç­†è¨˜æœ¬ä½¿ç”¨ **Llama-3.2-1B-Instruct** é€²è¡Œæ¸¬è©¦ã€‚\n",
                "é¸æ“‡æ­¤æ¨¡å‹çš„åŸå› ï¼š\n",
                "1. **æ¬Šé™é©—è­‰**ï¼šå®ƒåŒæ¨£æ˜¯ Meta çš„ Gated Repoï¼Œéœ€è¦ Hugging Face Token èˆ‡æˆæ¬Šï¼Œèƒ½å±•ç¾æ‚¨è™•ç†æ¨¡å‹å­˜å–çš„å°ˆæ¥­èƒ½åŠ›ã€‚\n",
                "2. **ç©©å®šæ€§**ï¼š1B æ¨¡å‹é«”ç©æ¥µå°ï¼Œä¿è­‰èƒ½åœ¨ Colab T4 å…è²»ç‰ˆé †æš¢åŸ·è¡Œï¼Œä¸æœƒç™¼ç”Ÿè¨˜æ†¶é«”å´©æ½°ã€‚\n",
                "3. **é«˜ TPS**ï¼šåœ¨ T4 ä¸Šèƒ½è·‘å‡ºéå¸¸äº®çœ¼çš„æ¨è«–é€Ÿåº¦æ•¸æ“šã€‚"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. å®‰è£å¥—ä»¶\n",
                "!pip install vllm pandas huggingface_hub"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. ç™»å…¥ Hugging Face (è«‹ä½¿ç”¨ Classic Token)\n",
                "from huggingface_hub import notebook_login\n",
                "notebook_login()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. åˆå§‹åŒ– vLLM (ä½¿ç”¨ 1B æ¨¡å‹)\n",
                "from vllm import LLM, SamplingParams\n",
                "import time\n",
                "\n",
                "# ä½¿ç”¨ Llama-3.2-1Bï¼Œé€™åœ¨ T4 ä¸Šéå¸¸ç©©å®š\n",
                "llm = LLM(model=\"meta-llama/Llama-3.2-1B-Instruct\", \n",
                "          dtype=\"half\", \n",
                "          gpu_memory_utilization=0.6, \n",
                "          max_model_len=2048)\n",
                "\n",
                "print(\"âœ… vLLM Engine initialized for Llama-3.2-1B!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. åŸ·è¡ŒåŸºæº–æ¸¬è©¦\n",
                "prompts = [\n",
                "    \"The future of autonomous agents is\",\n",
                "    \"Explain the concept of backpropagation in one paragraph.\",\n",
                "    \"Write a short story about a robot learning to cook.\",\n",
                "    \"How does LangGraph help developers build complex agents?\"\n",
                "] * 10  # 40 å€‹è«‹æ±‚ä»¥æ¸¬è©¦ç©©å®šæ€§\n",
                "\n",
                "sampling_params = SamplingParams(temperature=0.7, max_tokens=128)\n",
                "\n",
                "print(f\"ğŸš€ æ­£åœ¨è™•ç† {len(prompts)} å€‹è«‹æ±‚...\")\n",
                "start_time = time.time()\n",
                "outputs = llm.generate(prompts, sampling_params)\n",
                "end_time = time.time()\n",
                "\n",
                "total_duration = end_time - start_time\n",
                "total_tokens = sum([len(output.outputs[0].token_ids) for output in outputs])\n",
                "throughput = total_tokens / total_duration\n",
                "\n",
                "print(f\"\\n=== Benchmark Results ===\")\n",
                "print(f\"Total Tokens: {total_tokens}\")\n",
                "print(f\"Total Time: {total_duration:.2f} s\")\n",
                "print(f\"Throughput (TPS): {throughput:.2f} tokens/s\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}