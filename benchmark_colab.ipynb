{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Llama-3-8B 效能基準測試 (vLLM vs TGI Simulation)\n",
                "\n",
                "本筆記本用於在 Google Colab (T4 GPU) 上測試 vLLM 的實際推論速度。\n",
                "由於 Colab 免費版的資源限制，這裡主要演示 **vLLM** 的設定與效能測量。\n",
                "\n",
                "**注意**：Llama-3 模型需要 Hugging Face 存取權限，請確保您已在 Hugging Face 申請通過用且擁有 Token。"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 步驟 0：檢查 GPU\n",
                "請確保您已連線到 T4 GPU (Runtime -> Change runtime type -> T4 GPU)。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!nvidia-smi"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. 安裝必要套件\n",
                "# 重要：安裝過程可能會提示 protobuf版本衝突。\n",
                "# 安裝完成後，請務必點擊選單欄的【Runtime -> Restart session】重啟核心，然後再繼續往下執行！\n",
                "!pip install vllm pandas huggingface_hub"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 重要步驟：Hugging Face 授權\n",
                "使用的模型 `meta-llama/Meta-Llama-3-8B` 是受管制的 (Gated Model)。\n",
                "1. 請先前往 [Meta-Llama-3-8B 頁面](https://huggingface.co/meta-llama/Meta-Llama-3-8B) 申請存取權限並同意條款。\n",
                "2. 確保您的 Hugging Face Token 具有 `Read` 權限。\n",
                "3. 在下方登入時貼上該 Token。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. 登入 Hugging Face (貼上您的 HF Token)\n",
                "from huggingface_hub import notebook_login\n",
                "notebook_login()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. 準備測試數據\n",
                "import time\n",
                "from vllm import LLM, SamplingParams\n",
                "\n",
                "# 測試 prompts\n",
                "prompts = [\n",
                "    \"Hello, my name is\",\n",
                "    \"The president of the United States is\",\n",
                "    \"The capital of France is\",\n",
                "    \"The future of AI is\",\n",
                "    \"Python is a programming language that\",\n",
                "] * 4  # 重複幾次以增加負載\n",
                "\n",
                "sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=256)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. 初始化 vLLM 引擎 (使用 Llama-3-8B)\n",
                "# 注意：Colab T4 記憶體有限，我們使用 gpu_memory_utilization 控制顯存\n",
                "llm = LLM(model=\"meta-llama/Meta-Llama-3-8B\", \n",
                "          dtype=\"float16\", \n",
                "          gpu_memory_utilization=0.9,\n",
                "          max_model_len=4096)\n",
                "\n",
                "print(\"vLLM Engine initialized!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5. 執行基準測試\n",
                "start_time = time.time()\n",
                "\n",
                "outputs = llm.generate(prompts, sampling_params)\n",
                "\n",
                "end_time = time.time()\n",
                "total_duration = end_time - start_time\n",
                "\n",
                "# 計算統計數據\n",
                "total_tokens = sum([len(output.outputs[0].token_ids) for output in outputs])\n",
                "throughput = total_tokens / total_duration\n",
                "\n",
                "print(f\"\\n=== Benchmark Results ===\")\n",
                "print(f\"Total Requests: {len(prompts)}\")\n",
                "print(f\"Total Tokens Generated: {total_tokens}\")\n",
                "print(f\"Total Time: {total_duration:.2f} s\")\n",
                "print(f\"Throughput (TPS): {throughput:.2f} tokens/s\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}